{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve the annotated data\n",
    "\n",
    "We resolve the annotated data from BRAT to json (ner + re)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "\n",
    "1. Set `brat_data_dir` to the annotated data root dir\n",
    "2. Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brat_data_dir = \"/Users/liao/myProjects/repo/remote_brat/data/structured_reporting/ours/liao\"\n",
    "to_be_annotated_dir = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/to_be_annotated\"\n",
    "output_root_dir = \"./outputs/cxr_graph/json4ner_re\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(output_root_dir):\n",
    "    shutil.rmtree(output_root_dir)\n",
    "\n",
    "os.makedirs(output_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve BRAT result to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnEntityClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # T0\n",
    "        self.label = \"\"\n",
    "        self.start_index = -1  # include (char idx)\n",
    "        self.end_index = -1  # not include\n",
    "        self.token_str = \"\"\n",
    "        self.att_objs = []\n",
    "\n",
    "        self.id = \"\"  # E0\n",
    "        self.start_token_idx = -1 # include\n",
    "        self.end_token_idx = -1 # include\n",
    "        self.sent_idx = -1\n",
    "\n",
    "        self.type = \"\"  # ANAT, OBS, LOCATT\n",
    "        self.chain_info = {\n",
    "            \"modify\": {\"in\": [], \"out\": []},\n",
    "            \"part_of\": {\"in\": [], \"out\": []},\n",
    "            \"located_at\": {\"in\": [], \"out\": []},\n",
    "            \"suggestive_of\": {\"in\": [], \"out\": []},\n",
    "        }\n",
    "        self.resolve(stripped_str)\n",
    "        \n",
    "        self.abnormality = \"NA\"\n",
    "        self.action = \"NA\"\n",
    "        self.evolution = \"NA\"\n",
    "        \n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.brat_id}\\t{self.label} {self.start_index} {self.end_index}\\t{self.token_str}\\n\"\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(T\\d+)\\t(.+) (\\d+) (\\d+)\\t(.+)\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        obs_labels = [\"Observation-Present\", \"Observation-Absent\", \"Observation-Uncertain\"]\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, start_index, end_index, self.token_str = match_obj.groups()\n",
    "            self.start_index = int(start_index)\n",
    "            self.end_index = int(end_index)\n",
    "            if self.label in obs_labels:\n",
    "                self.type = \"OBS\"\n",
    "            elif self.label == \"Anatomy\":                \n",
    "                self.type = \"ANAT\"\n",
    "            elif self.label == \"Location-Attribute\":\n",
    "                self.type = \"LOCATT\"\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot identify: {self.label}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnEntityClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)\n",
    "\n",
    "\n",
    "class AnnRelationClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # R0\n",
    "        self.label = \"\"\n",
    "        self.arg1 = \"\"  # from entity: T0\n",
    "        self.arg2 = \"\"  # to entity: T1\n",
    "        self.resolve(stripped_str)\n",
    "\n",
    "        self.id = \"\"  # R0\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.brat_id}\\t{self.label} Arg1:{self.arg1} Arg2:{self.arg2}\\t\\n\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(R\\d+)\\t(.+) Arg1:(T\\d+) Arg2:(T\\d+)\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, self.arg1, self.arg2 = match_obj.groups()\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnRelationClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)\n",
    "\n",
    "\n",
    "class AnnAttributeClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # A0\n",
    "        self.label = \"\"\n",
    "        self.value = \"\"\n",
    "        self.target_entity_id = \"\"  # T0\n",
    "        \n",
    "        self.resolve(stripped_str)\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        if self.value:\n",
    "            return f\"{self.brat_id}\\t{self.label} {self.target_entity_id} {self.value}\"\n",
    "        else:\n",
    "            return f\"{self.brat_id}\\t{self.label} {self.target_entity_id}\"\n",
    "\n",
    "    def get_json_str(self) -> str:\n",
    "        if self.label == \"isAbnormal_OBS\":\n",
    "            return \"is_abnormal\"\n",
    "        if self.label == \"isNormal_OBS\":\n",
    "            return \"is_normal\"\n",
    "        if self.label == \"Uncertian_Tendency\":\n",
    "            return f\"uncertainty:{self.value}\"\n",
    "            raise ValueError(\"Should not have this attribute\")\n",
    "        if self.label == \"isRelative_Modifier\":\n",
    "            return f\"is_relative_modifier:{self.value}\"\n",
    "        if self.label == \"show_RelativeChange\":\n",
    "            return f\"has_relative_change:{self.value}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(A\\d+)\\t(.+) (T\\d+) ?(.+)?\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, self.target_entity_id, self.value = match_obj.groups()\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnAttributeClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id or other == self.target_entity_id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sent_relations = defaultdict(list)\n",
    "def bart2json(dataset_names, datasplits, output_name):\n",
    "    for dataset_name in dataset_names: # , \"CheXpert\"\n",
    "        for datasplit in datasplits:\n",
    "            for file_name in os.listdir(os.path.join(to_be_annotated_dir, dataset_name, \"label_in_use\", datasplit)):\n",
    "                if dataset_name == \"MIMIC-CXR\":\n",
    "                    doc_key = file_name.lstrip(f\"{dataset_name}_\").replace(\"_\", \"/\")\n",
    "                if dataset_name == \"CheXpert\":\n",
    "                    doc_key = file_name.lstrip(f\"{dataset_name}_\").rstrip(\".txt\")\n",
    "\n",
    "                txt_file_name = file_name\n",
    "                ann_file_name = f'{file_name.rstrip(\".txt\")}.ann'\n",
    "\n",
    "                txt_file = os.path.join(brat_data_dir, dataset_name, datasplit, txt_file_name)\n",
    "                ann_file = os.path.join(brat_data_dir, dataset_name, datasplit, ann_file_name)\n",
    "\n",
    "                output_dict = {\n",
    "                    \"doc_key\": doc_key,\n",
    "                    \"sentences\": [],\n",
    "                    \"ner\": [],\n",
    "                    \"relations\": [],\n",
    "                    \"entity_attributes\": [],\n",
    "                }\n",
    "\n",
    "                # 读取原始doc：只读取第一行\n",
    "                with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    doc_str = f.readline().strip()\n",
    "\n",
    "                # 超过这个范围的标签都应该排除（因为我们把RadGraph的标签也一起呈现给了标注者，所以解析时需要排除这些已有的标签）\n",
    "                valid_doc_len = len(doc_str)\n",
    "\n",
    "                # 读取标签\n",
    "                with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    ann_lines = f.readlines()\n",
    "                    # print(ann_lines)\n",
    "\n",
    "                ent_obj_list = []\n",
    "                rel_obj_list = []\n",
    "                att_obj_list = []\n",
    "                for ann_line in ann_lines:\n",
    "                    stripped_ann_line = ann_line.strip()\n",
    "                    if stripped_ann_line.startswith(\"T\"):\n",
    "                        ent = AnnEntityClass(stripped_ann_line)\n",
    "                        ent_obj_list.append(ent)\n",
    "                    elif stripped_ann_line.startswith(\"R\"):\n",
    "                        rel = AnnRelationClass(stripped_ann_line)\n",
    "                        rel_obj_list.append(rel)\n",
    "                    elif stripped_ann_line.startswith(\"A\"):\n",
    "                        att = AnnAttributeClass(stripped_ann_line)\n",
    "                        att_obj_list.append(att)\n",
    "                        ent = ent_obj_list[ent_obj_list.index(att.target_entity_id)]\n",
    "                        ent.att_objs.append(att)\n",
    "                        if att.label == \"isAbnormal_OBS\":\n",
    "                            ent.abnormality = \"Abnormal\"\n",
    "                        if att.label == \"isNormal_OBS\":\n",
    "                            ent.abnormality = \"Normal\"\n",
    "                        if att.label == \"isRelative_Modifier\":\n",
    "                            ent.action = att.value\n",
    "                        if att.label == \"show_RelativeChange\":\n",
    "                            ent.evolution = att.value\n",
    "                    else:\n",
    "                        raise ValueError(f\"Uncatched value from .ann file: {stripped_ann_line}\")\n",
    "                \n",
    "                ent_obj_list = list(filter(lambda ent: ent.start_index <= valid_doc_len and ent.end_index <= valid_doc_len, ent_obj_list))\n",
    "                rel_obj_list = list(filter(lambda rel: rel.arg1 in ent_obj_list and rel.arg2 in ent_obj_list, rel_obj_list))\n",
    "                att_obj_list = list(filter(lambda att: att.target_entity_id in ent_obj_list in ent_obj_list, att_obj_list))\n",
    "\n",
    "                # 识别token的位置，并添加token_idx; 按句子拆分\n",
    "                doc_tokens = doc_str.split(\" \")\n",
    "                token_start_idx_list = [] # token first char\n",
    "                token_end_idx_list = [] # token last char + 1\n",
    "                curr_start = 0\n",
    "                \n",
    "                sent_idx = 0\n",
    "                tokidx2sentidx = []\n",
    "                sent = []\n",
    "                for tok_idx, token_str in enumerate(doc_tokens):\n",
    "                    # 识别token的位置，并添加token_idx\n",
    "                    token_start_idx_list.append(curr_start)\n",
    "                    token_end_idx_list.append(curr_start + len(token_str))\n",
    "                    curr_start += len(token_str) + 1 # whitespace\n",
    "                    \n",
    "                    # 按句子拆分\n",
    "                    tokidx2sentidx.append(sent_idx)\n",
    "                    sent.append(token_str)\n",
    "                    if token_str == \".\" or tok_idx == len(doc_tokens) - 1:\n",
    "                        output_dict[\"sentences\"].append(sent)\n",
    "                        output_dict[\"ner\"].append([])\n",
    "                        output_dict[\"relations\"].append([])\n",
    "                        output_dict[\"entity_attributes\"].append([])\n",
    "                        sent_idx += 1\n",
    "                        sent = []\n",
    "                assert len(doc_tokens) == len([i for sent in output_dict[\"sentences\"] for i in sent])\n",
    "\n",
    "                for ent in ent_obj_list:\n",
    "                    ent.start_token_idx = token_start_idx_list.index(ent.start_index)\n",
    "                    ent.end_token_idx = token_end_idx_list.index(ent.end_index)\n",
    "                    assert ent.token_str == \" \".join(doc_tokens[ent.start_token_idx : ent.end_token_idx + 1])\n",
    "                    \n",
    "                    starttok_sent_idx = tokidx2sentidx[ent.start_token_idx]\n",
    "                    endtok_sent_idx = tokidx2sentidx[ent.end_token_idx]\n",
    "                    ent.sent_idx = starttok_sent_idx\n",
    "                    assert starttok_sent_idx == endtok_sent_idx\n",
    "\n",
    "                # Entity\n",
    "                for ent_id, ent in enumerate(sorted(ent_obj_list, key=lambda x: x.start_token_idx)):\n",
    "                    output_dict[\"ner\"][ent.sent_idx].append([ent.start_token_idx, ent.end_token_idx, ent.label])\n",
    "                    \n",
    "                    # Attribute\n",
    "                    if ent.att_objs:\n",
    "                        output_dict[\"entity_attributes\"][ent.sent_idx].append([ent.start_token_idx, ent.end_token_idx, ent.abnormality, ent.action, ent.evolution])\n",
    "\n",
    "                # Relation\n",
    "                for rel_id, rel in enumerate(sorted(rel_obj_list, key=lambda x: ent_obj_list[ent_obj_list.index(x.arg1)].start_token_idx)):\n",
    "                    subj = ent_obj_list[ent_obj_list.index(rel.arg1)]\n",
    "                    obj = ent_obj_list[ent_obj_list.index(rel.arg2)]\n",
    "                    output_dict[\"relations\"][subj.sent_idx].append([subj.start_token_idx, subj.end_token_idx, obj.start_token_idx, obj.end_token_idx, rel.label])\n",
    "                    cross_sent_relations[f\"{dataset_name}_{datasplit}_{output_name}\"].append(abs(subj.sent_idx - obj.sent_idx))\n",
    "\n",
    "                output_path = os.path.join(output_root_dir, output_name)\n",
    "                with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(output_dict))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart2json(dataset_names=[\"MIMIC-CXR\"], datasplits=[\"train\"], output_name=\"train.json\")\n",
    "bart2json(dataset_names=[\"MIMIC-CXR\"], datasplits=[\"dev\"], output_name=\"dev.json\")\n",
    "bart2json(dataset_names=[\"MIMIC-CXR\", \"CheXpert\"], datasplits=[\"test\"], output_name=\"test.json\")\n",
    "bart2json(dataset_names=[\"MIMIC-CXR\"], datasplits=[\"test\"], output_name=\"test_mimic.json\")\n",
    "bart2json(dataset_names=[\"CheXpert\"], datasplits=[\"test\"], output_name=\"test_chexpert.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC-CXR_train_train.json\n",
      "Cross sentence relations: (|subj_sentid - obj_sentid|, num_relation_pairs):\n",
      "[(0, 9554), (1, 66)]\n",
      "\n",
      "MIMIC-CXR_dev_dev.json\n",
      "Cross sentence relations: (|subj_sentid - obj_sentid|, num_relation_pairs):\n",
      "[(0, 1706), (1, 11)]\n",
      "\n",
      "MIMIC-CXR_test_test.json\n",
      "Cross sentence relations: (|subj_sentid - obj_sentid|, num_relation_pairs):\n",
      "[(0, 984), (1, 3)]\n",
      "\n",
      "CheXpert_test_test.json\n",
      "Cross sentence relations: (|subj_sentid - obj_sentid|, num_relation_pairs):\n",
      "[(0, 1138), (1, 12), (2, 1)]\n",
      "\n",
      "MIMIC-CXR_test_test_mimic.json\n",
      "Cross sentence relations: (|subj_sentid - obj_sentid|, num_relation_pairs):\n",
      "[(0, 984), (1, 3)]\n",
      "\n",
      "CheXpert_test_test_chexpert.json\n",
      "Cross sentence relations: (|subj_sentid - obj_sentid|, num_relation_pairs):\n",
      "[(0, 1138), (1, 12), (2, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for k, v in cross_sent_relations.items():\n",
    "    print(k)\n",
    "    c = Counter(v)\n",
    "    print(\"Cross sentence relations: (|subj_sentid - obj_sentid|, num_relation_pairs):\")\n",
    "    print(c.most_common())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
