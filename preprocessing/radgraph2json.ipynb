{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# the raw data downloaded from https://physionet.org/content/radgraph/1.0.0/\n",
    "root_path = \"/Users/liao/Desktop/RadGraph/radgraph-extracting-clinical-entities-and-relations-from-radiology-reports-1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# the output dir\n",
    "output_dir = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/radgraph/json4ner_re\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sent_relations = defaultdict(list)\n",
    "\n",
    "for out_file_name in [\"train\", \"dev\", \"test\", \"test1\"]:\n",
    "    if out_file_name == \"test1\":\n",
    "        in_file_name = \"test\"\n",
    "    else:\n",
    "        in_file_name = out_file_name\n",
    "\n",
    "    input_file_path = os.path.join(root_path, f\"{in_file_name}.json\")\n",
    "    with open(input_file_path, \"r\") as f:\n",
    "        docs_dict = json.loads(f.readline())\n",
    "\n",
    "    for doc_key, doc in docs_dict.items():\n",
    "        output_dict = {\n",
    "            \"doc_key\": doc_key,\n",
    "            \"sentences\": [],\n",
    "            \"ner\": [],\n",
    "            \"relations\": [],\n",
    "        }\n",
    "        sent_idx = 0\n",
    "        tokidx2sentidx = []\n",
    "        tokens = doc[\"text\"].split(\" \")\n",
    "        sent = []\n",
    "        for token_id, token in enumerate(tokens):\n",
    "            tokidx2sentidx.append(sent_idx)\n",
    "            sent.append(token)\n",
    "            if token == \".\" or token_id == len(tokens) - 1:\n",
    "                output_dict[\"sentences\"].append(sent)\n",
    "                output_dict[\"ner\"].append([])\n",
    "                output_dict[\"relations\"].append([])\n",
    "                sent_idx += 1\n",
    "                sent = []\n",
    "        assert len(tokens) == len([i for sent in output_dict[\"sentences\"] for i in sent])\n",
    "\n",
    "        if out_file_name == \"test\":\n",
    "            doc_entities = doc[\"labeler_1\"][\"entities\"]\n",
    "        elif out_file_name == \"test1\":\n",
    "            doc_entities = doc[\"labeler_2\"][\"entities\"]\n",
    "        else:\n",
    "            doc_entities = doc[\"entities\"]\n",
    "            \n",
    "        dataset=\"mimic\"\n",
    "        if \"txt\" not in doc_key and \"test\" in out_file_name:\n",
    "            dataset=\"chexpert\"\n",
    "\n",
    "        for ent_idx, entity in doc_entities.items():\n",
    "            subj_start = entity[\"start_ix\"]\n",
    "            subj_end = entity[\"end_ix\"]\n",
    "            subj_sent_idx = tokidx2sentidx[subj_start]\n",
    "            output_dict[\"ner\"][subj_sent_idx].append([subj_start, subj_end, entity[\"label\"]])\n",
    "            for rel in entity[\"relations\"]:\n",
    "                rel_label = rel[0]\n",
    "                rel_obj_idx = rel[1]\n",
    "                obj_start = doc_entities[rel_obj_idx][\"start_ix\"]\n",
    "                obj_end = doc_entities[rel_obj_idx][\"end_ix\"]\n",
    "                obj_sent_idx = tokidx2sentidx[obj_start]\n",
    "                cross_sent_relations[f\"{dataset}_{out_file_name}\"].append(abs(subj_sent_idx - obj_sent_idx))\n",
    "                output_dict[\"relations\"][subj_sent_idx].append([subj_start, subj_end, obj_start, obj_end, rel_label])\n",
    "                # if abs(subj_sent_idx - obj_sent_idx) > 0:\n",
    "                #     print(doc_key, entity)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"{out_file_name}.json\")\n",
    "        with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(output_dict))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "        if \"txt\" in doc_key and \"test\" in out_file_name:\n",
    "            output_path = os.path.join(output_dir, f\"{out_file_name}_mimic.json\")\n",
    "            with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(output_dict))\n",
    "                f.write(\"\\n\")\n",
    "        if \"txt\" not in doc_key and \"test\" in out_file_name:\n",
    "            output_path = os.path.join(output_dir, f\"{out_file_name}_chexpert.json\")\n",
    "            with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(output_dict))\n",
    "                f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
