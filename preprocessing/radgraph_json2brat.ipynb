{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data for annotation\n",
    "\n",
    "We are annotating the same data as RadGraph.\n",
    "\n",
    "Prepare:\n",
    "1. Set `radgraph_root_dir` to the downloaded RadGraph root dir\n",
    "2. Run the script\n",
    "\n",
    "Outputs:\n",
    "1. `brat_data/..` is the files for `the BRAT Annotation Tool`, relavant config files are in `graph_annotation_process/brat_config`\n",
    "2. `label_in_use/..` is the files that store existing labels before annotation. We use these to distinguish which labels are created in annotation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "radgraph_root_dir = \"/Users/liao/Desktop/RadGraph/radgraph-extracting-clinical-entities-and-relations-from-radiology-reports-1.0.0\"\n",
    "output_root_dir = \"./outputs/to_be_annotated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_list(doc_tokens_list):\n",
    "    pos_list = []\n",
    "    curr_pos = 0\n",
    "    for tok in doc_tokens_list:\n",
    "        pos_list.append(curr_pos)\n",
    "        curr_pos = curr_pos + len(tok)+1\n",
    "    return pos_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnEntityClass:\n",
    "    def __init__(self,incremental_id) -> None:\n",
    "        self.id = f\"T{next(incremental_id)}\"\n",
    "        self.label = \"\"\n",
    "        self.start_index = \"\"\n",
    "        self.end_index = \"\"\n",
    "        self.token_str = \"\"\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.label} {self.start_index} {self.end_index}\\t{self.token_str}\\n\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "\n",
    "class AnnRelationClass:\n",
    "    __incremental_id = itertools.count(1)\n",
    "\n",
    "    def __init__(self,incremental_id) -> None:\n",
    "        self.id = f\"R{next(incremental_id)}\"\n",
    "        self.label = \"\"\n",
    "        self.arg1 = \"\"\n",
    "        self.arg2 = \"\"\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.id}\\t{self.label} Arg1:{self.arg1} Arg2:{self.arg2}\\t\\n\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert test.json to brat format\n",
    "\n",
    "Having two annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_path = os.path.join(radgraph_root_dir, \"test.json\")\n",
    "\n",
    "with open(input_test_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    radgraph_test = f.readlines()\n",
    "data_dict = json.loads(\"\".join(radgraph_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_mapper: dict[str, str] = {\"ANAT-DP\": \"Anatomy\", \"OBS-DP\": \"Observation-Present\", \"OBS-DA\": \"Observation-Absent\", \"OBS-U\": \"Observation-Uncertain\"}\n",
    "relation_label_mapper: dict[str, str] = {\"modify\": \"modify\", \"suggestive_of\": \"suggestive_of\", \"located_at\": \"located_at\"}\n",
    "\n",
    "\n",
    "for doc_id, doc_info in data_dict.items():\n",
    "    output_file_name = f'{doc_info[\"data_source\"]}_{doc_id.replace(\"/\",\"_\").strip(\".txt\")}'\n",
    "    \n",
    "    output_dir = os.path.join(output_root_dir, doc_info[\"data_source\"], \"brat_data\", doc_info[\"data_split\"])\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # We want to show the unlabelled data and two labelers' result in the same file. \n",
    "    # So we need to offset two labelers' token pos\n",
    "    doc_tokens_list = doc_info[\"text\"].split(\" \")\n",
    "    doc_tokens_pos_list0 = get_pos_list(doc_tokens_list)\n",
    "    doc_tokens_pos_list1 = [i+len(doc_info[\"text\"])+2 for i in doc_tokens_pos_list0]\n",
    "    doc_tokens_pos_list2 = [i+len(doc_info[\"text\"])+2 for i in doc_tokens_pos_list1]\n",
    "    pos_list_mapper:dict[str, list] = {\"labeler_1\":doc_tokens_pos_list1, \"labeler_2\":doc_tokens_pos_list2}\n",
    "\n",
    "    # Create txt file\n",
    "    with open(os.path.join(output_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(doc_info[\"text\"]+\"\\n\\n\"+doc_info[\"text\"]+\"\\n\\n\"+doc_info[\"text\"])\n",
    "\n",
    "    # Create ann file\n",
    "    ann_entitiy_list: list[AnnEntityClass] = []\n",
    "    ann_relation_list: list[AnnRelationClass] = []\n",
    "    \n",
    "    entity_incremental_id = itertools.count(1)\n",
    "    relation_incremental_id = itertools.count(1)\n",
    "    for labeler_id, doc_tokens_pos_list in pos_list_mapper.items():\n",
    "        # Temperarly save the entiity relations, since one entity may link forward to another unrecorded entity.\n",
    "        # (A, modify, B) == A -> modify -> B\n",
    "        temp_ann_entitiy_dict: dict[str,AnnEntityClass] = {}\n",
    "        temp_raw_relation_list: list[tuple[str,str,str]] = []\n",
    "\n",
    "        entities_dict = doc_info[labeler_id][\"entities\"]\n",
    "        for entity_id, entity_info in entities_dict.items():\n",
    "            ann_entity = AnnEntityClass(entity_incremental_id)\n",
    "            ann_entity.label = entity_label_mapper[entity_info[\"label\"]]\n",
    "            ann_entity.token_str = entity_info[\"tokens\"]\n",
    "            ann_entity.start_index = doc_tokens_pos_list[entity_info[\"start_ix\"]]\n",
    "            ann_entity.end_index = ann_entity.start_index + len(ann_entity.token_str)\n",
    "            for relation in entity_info[\"relations\"]:\n",
    "                temp_raw_relation_list.append((entity_id, relation[0], relation[1]))\n",
    "            temp_ann_entitiy_dict[entity_id] = ann_entity\n",
    "        ann_entitiy_list.extend(temp_ann_entitiy_dict.values())\n",
    "        \n",
    "        for entity1_idstr, relation_label, entity2_idstr in temp_raw_relation_list:\n",
    "            ann_relation = AnnRelationClass(relation_incremental_id)\n",
    "            ann_relation.label = relation_label\n",
    "            ann_relation.arg1 = temp_ann_entitiy_dict[entity1_idstr].id\n",
    "            ann_relation.arg2 = temp_ann_entitiy_dict[entity2_idstr].id\n",
    "            ann_relation_list.append(ann_relation)\n",
    "    \n",
    "    # Dir to save the labels' id that are created for showing only, \n",
    "    # which should be ignored when resolving the finial ann data.\n",
    "    label_inUse_dir = os.path.join(output_root_dir, doc_info[\"data_source\"], \"label_in_use\", doc_info[\"data_split\"])\n",
    "    if not os.path.exists(label_inUse_dir):\n",
    "        os.makedirs(label_inUse_dir)\n",
    "\n",
    "    with open(os.path.join(output_dir, output_file_name+\".ann\"), \"w\", encoding=\"utf-8\") as f1, \\\n",
    "        open(os.path.join(label_inUse_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f2:\n",
    "        for label_obj in ann_entitiy_list+ann_relation_list:\n",
    "            f1.write(label_obj.get_ann_str())\n",
    "            f2.write(str(label_obj.id)+\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert train/dev.json to brat format\n",
    "\n",
    "Having one annotator, do not pre-annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_mapper: dict[str, str] = {\"ANAT-DP\": \"Anatomy\", \"OBS-DP\": \"Observation-Present\", \"OBS-DA\": \"Observation-Absent\", \"OBS-U\": \"Observation-Uncertain\"}\n",
    "relation_label_mapper: dict[str, str] = {\"modify\": \"modify\", \"suggestive_of\": \"suggestive_of\", \"located_at\": \"located_at\"}\n",
    "\n",
    "input_dev_path = os.path.join(radgraph_root_dir, \"dev.json\")\n",
    "input_train_path = os.path.join(radgraph_root_dir, \"train.json\")\n",
    "\n",
    "for input_file_path in [input_train_path, input_dev_path]:\n",
    "    with open(input_file_path,\"r\",encoding=\"utf-8\") as f:\n",
    "        radgraph_test = f.readlines()\n",
    "        data_dict = json.loads(\"\".join(radgraph_test))\n",
    "\n",
    "    for doc_id, doc_info in data_dict.items():\n",
    "        output_file_name = f'{doc_info[\"data_source\"]}_{doc_id.replace(\"/\",\"_\").strip(\".txt\")}'\n",
    "        \n",
    "        output_dir = os.path.join(output_root_dir, doc_info[\"data_source\"], \"brat_data\", doc_info[\"data_split\"])\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        doc_tokens_list = doc_info[\"text\"].split(\" \")\n",
    "        doc_tokens_pos_list = [i+len(doc_info[\"text\"])+2 for i in get_pos_list(doc_tokens_list)]\n",
    "\n",
    "        # Create txt file\n",
    "        with open(os.path.join(output_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(doc_info[\"text\"]+\"\\n\\n\"+doc_info[\"text\"])\n",
    "\n",
    "        # Create ann file\n",
    "        ann_entitiy_list: list[AnnEntityClass] = []\n",
    "        ann_relation_list: list[AnnRelationClass] = []\n",
    "        \n",
    "        entity_incremental_id = itertools.count(1)\n",
    "        relation_incremental_id = itertools.count(1)\n",
    "        \n",
    "        # Temperarly save the entiity relations, since one entity may link forward to another unrecorded entity.\n",
    "        # (A, modify, B) == A -> modify -> B\n",
    "        temp_ann_entitiy_dict: dict[str,AnnEntityClass] = {}\n",
    "        temp_raw_relation_list: list[tuple[str,str,str]] = []\n",
    "\n",
    "        entities_dict = doc_info[\"entities\"]\n",
    "        for entity_id, entity_info in entities_dict.items():\n",
    "            ann_entity = AnnEntityClass(entity_incremental_id)\n",
    "            ann_entity.label = entity_label_mapper[entity_info[\"label\"]]\n",
    "            ann_entity.token_str = entity_info[\"tokens\"]\n",
    "            ann_entity.start_index = doc_tokens_pos_list[entity_info[\"start_ix\"]]\n",
    "            ann_entity.end_index = ann_entity.start_index + len(ann_entity.token_str)\n",
    "            for relation in entity_info[\"relations\"]:\n",
    "                temp_raw_relation_list.append((entity_id, relation[0], relation[1]))\n",
    "            temp_ann_entitiy_dict[entity_id] = ann_entity\n",
    "        ann_entitiy_list.extend(temp_ann_entitiy_dict.values())\n",
    "        \n",
    "        for entity1_idstr, relation_label, entity2_idstr in temp_raw_relation_list:\n",
    "            ann_relation = AnnRelationClass(relation_incremental_id)\n",
    "            ann_relation.label = relation_label\n",
    "            ann_relation.arg1 = temp_ann_entitiy_dict[entity1_idstr].id\n",
    "            ann_relation.arg2 = temp_ann_entitiy_dict[entity2_idstr].id\n",
    "            ann_relation_list.append(ann_relation)\n",
    "\n",
    "        # Dir to save the labels' id that are created for showing only, \n",
    "        # which should be ignored when resolving the finial ann data.\n",
    "        label_inUse_dir = os.path.join(output_root_dir, doc_info[\"data_source\"], \"label_in_use\", doc_info[\"data_split\"])\n",
    "        if not os.path.exists(label_inUse_dir):\n",
    "            os.makedirs(label_inUse_dir)\n",
    "\n",
    "        with open(os.path.join(output_dir, output_file_name+\".ann\"), \"w\", encoding=\"utf-8\") as f1, \\\n",
    "            open(os.path.join(label_inUse_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f2:\n",
    "            for label_obj in ann_entitiy_list+ann_relation_list:\n",
    "                f1.write(label_obj.get_ann_str())\n",
    "                f2.write(str(label_obj.id)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having one annotator, giving repeated pre-annotated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_label_mapper: dict[str, str] = {\"ANAT-DP\": \"Anatomy\", \"OBS-DP\": \"Observation-Present\", \"OBS-DA\": \"Observation-Absent\", \"OBS-U\": \"Observation-Uncertain\"}\n",
    "# relation_label_mapper: dict[str, str] = {\"modify\": \"modify\", \"suggestive_of\": \"suggestive_of\", \"located_at\": \"located_at\"}\n",
    "\n",
    "\n",
    "# input_dev_path = os.path.join(radgraph_root_dir, \"dev.json\")\n",
    "# input_train_path = os.path.join(radgraph_root_dir, \"train.json\")\n",
    "\n",
    "# for input_file_path in [input_train_path, input_dev_path]:\n",
    "#     with open(input_file_path,\"r\",encoding=\"utf-8\") as f:\n",
    "#         radgraph_test = f.readlines()\n",
    "#         data_dict = json.loads(\"\".join(radgraph_test))\n",
    "\n",
    "#     for doc_id, doc_info in data_dict.items():\n",
    "#         output_file_name = f'{doc_info[\"data_source\"]}_{doc_id.replace(\"/\",\"_\").strip(\".txt\")}'\n",
    "        \n",
    "#         output_dir = os.path.join(output_root_dir, doc_info[\"data_source\"], \"brat_data\", doc_info[\"data_split\"])\n",
    "#         if not os.path.exists(output_dir):\n",
    "#             os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "#         # We want to show the unlabelled data and two labelers' result in the same file. \n",
    "#         # So we need to offset two labelers' token pos\n",
    "#         doc_tokens_list = doc_info[\"text\"].split(\" \")\n",
    "#         doc_tokens_pos_list0 = get_pos_list(doc_tokens_list)\n",
    "#         doc_tokens_pos_list1 = [i+len(doc_info[\"text\"])+2 for i in doc_tokens_pos_list0]\n",
    "#         pos_list_mapper:dict[str, list] = {\"keep\":doc_tokens_pos_list1, \"remove\":doc_tokens_pos_list0}\n",
    "\n",
    "#         # Create txt file\n",
    "#         with open(os.path.join(output_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "#             f.write(doc_info[\"text\"]+\"\\n\\n\"+doc_info[\"text\"])\n",
    "\n",
    "#         # For ann file\n",
    "#         ann_entitiy_list: list[AnnEntityClass] = []\n",
    "#         ann_relation_list: list[AnnRelationClass] = []\n",
    "        \n",
    "#         # For label_in_use txt file\n",
    "#         labelinuse_ann_entitiy_list: list[AnnEntityClass] = []\n",
    "#         labelinuse_ann_relation_list: list[AnnRelationClass] = []\n",
    "        \n",
    "#         entity_incremental_id = itertools.count(1)\n",
    "#         relation_incremental_id = itertools.count(1)\n",
    "#         for action, doc_tokens_pos_list in pos_list_mapper.items():\n",
    "#             # Temperarly save the entiity relations, since one entity may link forward to another unrecorded entity.\n",
    "#             # (A, modify, B) == A -> modify -> B\n",
    "#             temp_ann_entitiy_dict: dict[str,AnnEntityClass] = {}\n",
    "#             temp_raw_relation_list: list[tuple[str,str,str]] = []\n",
    "\n",
    "#             entities_dict = doc_info[\"entities\"]\n",
    "#             for entity_id, entity_info in entities_dict.items():\n",
    "#                 ann_entity = AnnEntityClass(entity_incremental_id)\n",
    "#                 ann_entity.label = entity_label_mapper[entity_info[\"label\"]]\n",
    "#                 ann_entity.token_str = entity_info[\"tokens\"]\n",
    "#                 ann_entity.start_index = doc_tokens_pos_list[entity_info[\"start_ix\"]]\n",
    "#                 ann_entity.end_index = ann_entity.start_index + len(ann_entity.token_str)\n",
    "#                 for relation in entity_info[\"relations\"]:\n",
    "#                     temp_raw_relation_list.append((entity_id, relation[0], relation[1]))\n",
    "#                 temp_ann_entitiy_dict[entity_id] = ann_entity\n",
    "#             ann_entitiy_list.extend(temp_ann_entitiy_dict.values())\n",
    "#             if action == \"keep\":\n",
    "#                 labelinuse_ann_entitiy_list.extend(temp_ann_entitiy_dict.values())\n",
    "            \n",
    "#             for entity1_idstr, relation_label, entity2_idstr in temp_raw_relation_list:\n",
    "#                 ann_relation = AnnRelationClass(relation_incremental_id)\n",
    "#                 ann_relation.label = relation_label\n",
    "#                 ann_relation.arg1 = temp_ann_entitiy_dict[entity1_idstr].id\n",
    "#                 ann_relation.arg2 = temp_ann_entitiy_dict[entity2_idstr].id\n",
    "#                 ann_relation_list.append(ann_relation)\n",
    "#                 if action == \"keep\":\n",
    "#                     labelinuse_ann_relation_list.append(ann_relation)\n",
    "        \n",
    "#         # Dir to save the labels' id that are created for showing only, \n",
    "#         # which should be ignored when resolving the finial ann data.\n",
    "#         label_inUse_dir = os.path.join(output_root_dir, doc_info[\"data_source\"], \"label_in_use\", doc_info[\"data_split\"])\n",
    "#         if not os.path.exists(label_inUse_dir):\n",
    "#             os.makedirs(label_inUse_dir)\n",
    "\n",
    "#         with open(os.path.join(output_dir, output_file_name+\".ann\"), \"w\", encoding=\"utf-8\") as f1:\n",
    "#             for label_obj in ann_entitiy_list + ann_relation_list:\n",
    "#                 f1.write(label_obj.get_ann_str())\n",
    "                \n",
    "#         with open(os.path.join(label_inUse_dir, output_file_name+\".txt\"), \"w\", encoding=\"utf-8\") as f2:\n",
    "#             for label_obj in labelinuse_ann_entitiy_list + labelinuse_ann_relation_list:\n",
    "#                 f2.write(str(label_obj.id)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
