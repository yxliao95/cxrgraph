{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve and convert our CXRGraph to the same scheme as RadGraph\n",
    "\n",
    "Resolve the annotated data from BRAT and downgrade our CXRGraph to RadGraph for a direct comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "brat_data_dir = \"/Users/liao/myProjects/repo/remote_brat/data/structured_reporting/ours/liao\"\n",
    "to_be_annotated_dir = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/to_be_annotated\"\n",
    "cxrgraph_output_dir = \"./outputs/cxr_graph/json4ner_re_radraph_version\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(cxrgraph_output_dir):\n",
    "    shutil.rmtree(cxrgraph_output_dir)\n",
    "\n",
    "os.makedirs(cxrgraph_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve BRAT result to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnEntityClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # T0\n",
    "        self.label = \"\"\n",
    "        self.start_index = -1  # include (char idx)\n",
    "        self.end_index = -1  # not include\n",
    "        self.token_str = \"\"\n",
    "        self.att_objs = []\n",
    "        \n",
    "        self.id = \"\" # E0\n",
    "        self.start_token_idx = -1 # include\n",
    "        self.end_token_idx = -1 # include\n",
    "        self.sent_idx = -1\n",
    "        \n",
    "        self.type = \"\"  # ANAT, OBS, LOCATT\n",
    "        self.chain_info = {\n",
    "            \"modify\": {\"in\": [], \"out\": []},\n",
    "            \"part_of\": {\"in\": [], \"out\": []},\n",
    "            \"located_at\": {\"in\": [], \"out\": []},\n",
    "            \"suggestive_of\": {\"in\": [], \"out\": []},\n",
    "        }\n",
    "        self.resolve(stripped_str)\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"({self.start_token_idx},{self.end_token_idx}):{self.brat_id}\\t{self.label} {self.start_index} {self.end_index}\\t{self.token_str}\\n\"\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(T\\d+)\\t(.+) (\\d+) (\\d+)\\t(.+)\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, start_index, end_index, self.token_str = match_obj.groups()\n",
    "            self.start_index = int(start_index)\n",
    "            self.end_index = int(end_index)\n",
    "            if self.label in [\"Observation-Present\", \"Observation-Absent\", \"Observation-Uncertain\"]:\n",
    "                self.type = \"OBS\"\n",
    "            elif self.label == \"Anatomy\":\n",
    "                self.type = \"ANAT\"\n",
    "            elif self.label == \"Location-Attribute\":\n",
    "                self.type = \"LOCATT\"\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot identify: {self.label}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnEntityClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)\n",
    "        \n",
    "\n",
    "class AnnRelationClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # R0\n",
    "        self.label = \"\"\n",
    "        self.arg1 = \"\"  # from entity: T0\n",
    "        self.arg2 = \"\"  # to entity: T1\n",
    "        self.resolve(stripped_str)\n",
    "        \n",
    "        self.id = \"\" # R0\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.brat_id}\\t{self.label} Arg1:{self.arg1} Arg2:{self.arg2}\\t\\n\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(R\\d+)\\t(.+) Arg1:(T\\d+) Arg2:(T\\d+)\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, self.arg1, self.arg2 = match_obj.groups()\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnRelationClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)\n",
    "\n",
    "\n",
    "class AnnAttributeClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # A0\n",
    "        self.label = \"\"\n",
    "        self.value = \"\"\n",
    "        self.target_entity_id = \"\"  # T0\n",
    "        self.resolve(stripped_str)\n",
    "        \n",
    "    def get_ann_str(self) -> str:\n",
    "        if self.value:\n",
    "            return f\"{self.brat_id}\\t{self.label} {self.target_entity_id} {self.value}\"\n",
    "        else:\n",
    "            return f\"{self.brat_id}\\t{self.label} {self.target_entity_id}\"\n",
    "    \n",
    "    def get_json_str(self) -> str:\n",
    "        if self.label == \"isAbnormal_OBS\":\n",
    "            return \"is_abnormal\"\n",
    "        if self.label == \"isNormal_OBS\":\n",
    "            return \"is_normal\"\n",
    "        if self.label == \"Uncertian_Tendency\":\n",
    "            return f\"uncertainy:{self.value}\"\n",
    "        if self.label == \"isRelative_Modifier\":\n",
    "            return f\"is_relative_modifier:{self.value}\"\n",
    "        if self.label == \"show_RelativeChange\":\n",
    "            return f\"has_relative_change:{self.value}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(A\\d+)\\t(.+) (T\\d+) ?(.+)?\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, self.target_entity_id, self.value = match_obj.groups()\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnAttributeClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id or other == self.target_entity_id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart2json(dataset_name, datasplit):\n",
    "    if dataset_name == \"MIMIC-CXR\":\n",
    "        doc_key = file_name.lstrip(f\"{dataset_name}_\").replace(\"_\", \"/\")\n",
    "    else:\n",
    "        doc_key = file_name.lstrip(f\"{dataset_name}_\").rstrip(\".txt\")\n",
    "    \n",
    "    # 加载数据：\n",
    "    txt_file_name = file_name\n",
    "    ann_file_name = f'{file_name.rstrip(\".txt\")}.ann'\n",
    "\n",
    "    txt_file = os.path.join(brat_data_dir, dataset_name, datasplit, txt_file_name)\n",
    "    ann_file = os.path.join(brat_data_dir, dataset_name, datasplit, ann_file_name)\n",
    "\n",
    "    output_dict = {\n",
    "        \"doc_key\": doc_key,\n",
    "        \"sentences\": [],\n",
    "        \"ner\": [],\n",
    "        \"relations\": [],\n",
    "    }\n",
    "\n",
    "    # 读取原始doc：只读取第一行\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc_str = f.readline().strip()\n",
    "\n",
    "    # 超过这个范围的标签都应该排除（因为我们把RadGraph的标签也一起呈现给了标注者，所以解析时需要排除这些已有的标签）\n",
    "    valid_doc_len = len(doc_str)\n",
    "\n",
    "    # 读取标签\n",
    "    with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        ann_lines = f.readlines()\n",
    "        # print(ann_lines)\n",
    "\n",
    "\n",
    "    ent_obj_list = []\n",
    "    rel_obj_list = []\n",
    "    att_obj_list = []\n",
    "    for ann_line in ann_lines:\n",
    "        stripped_ann_line = ann_line.strip()\n",
    "        if stripped_ann_line.startswith(\"T\"):\n",
    "            ent = AnnEntityClass(stripped_ann_line)\n",
    "            ent_obj_list.append(ent)\n",
    "        elif stripped_ann_line.startswith(\"R\"):\n",
    "            rel = AnnRelationClass(stripped_ann_line)\n",
    "            rel_obj_list.append(rel)\n",
    "        elif stripped_ann_line.startswith(\"A\"):\n",
    "            att = AnnAttributeClass(stripped_ann_line)\n",
    "            att_obj_list.append(att)\n",
    "            ent = ent_obj_list[ent_obj_list.index(att.target_entity_id)]\n",
    "            ent.att_objs.append(att)\n",
    "        else:\n",
    "            raise ValueError(f\"Uncatched value from .ann file: {stripped_ann_line}\")\n",
    "    \n",
    "    ent_obj_list = list(filter(lambda ent: ent.start_index <= valid_doc_len and ent.end_index <= valid_doc_len, ent_obj_list))\n",
    "    rel_obj_list = list(filter(lambda rel: rel.arg1 in ent_obj_list and rel.arg2 in ent_obj_list, rel_obj_list))\n",
    "    att_obj_list = list(filter(lambda att: att.target_entity_id in ent_obj_list in ent_obj_list, att_obj_list))\n",
    "\n",
    "    # 识别token的位置，并添加token_idx; 按句子拆分\n",
    "    doc_tokens = doc_str.split(\" \")\n",
    "    token_start_idx_list = [] # token first char\n",
    "    token_end_idx_list = [] # token last char + 1\n",
    "    curr_start = 0\n",
    "\n",
    "    sent_idx = 0\n",
    "    tokidx2sentidx = []\n",
    "    sent = []\n",
    "    for tok_idx, token_str in enumerate(doc_tokens):\n",
    "        # 识别token的位置，并添加token_idx\n",
    "        token_start_idx_list.append(curr_start)\n",
    "        token_end_idx_list.append(curr_start + len(token_str))\n",
    "        curr_start += len(token_str) + 1 # whitespace\n",
    "        \n",
    "        # 按句子拆分\n",
    "        tokidx2sentidx.append(sent_idx)\n",
    "        sent.append(token_str)\n",
    "        if token_str == \".\" or tok_idx == len(doc_tokens) - 1:\n",
    "            output_dict[\"sentences\"].append(sent)\n",
    "            output_dict[\"ner\"].append([])\n",
    "            output_dict[\"relations\"].append([])\n",
    "            sent_idx += 1\n",
    "            sent = []\n",
    "    assert len(doc_tokens) == len([i for sent in output_dict[\"sentences\"] for i in sent])\n",
    "\n",
    "    for ent in ent_obj_list:\n",
    "        ent.start_token_idx = token_start_idx_list.index(ent.start_index)\n",
    "        ent.end_token_idx = token_end_idx_list.index(ent.end_index)\n",
    "        assert ent.token_str == \" \".join(doc_tokens[ent.start_token_idx:ent.end_token_idx + 1])\n",
    "        \n",
    "        starttok_sent_idx = tokidx2sentidx[ent.start_token_idx]\n",
    "        endtok_sent_idx = tokidx2sentidx[ent.end_token_idx]\n",
    "        ent.sent_idx = starttok_sent_idx\n",
    "        assert starttok_sent_idx == endtok_sent_idx\n",
    "\n",
    "    # 生成跟RadGraph一样的数据，然后评估\n",
    "    \n",
    "    # Entity\n",
    "    entity_label_mapper= {\"Anatomy\":\"ANAT-DP\", \"Observation-Present\":\"OBS-DP\", \"Observation-Absent\":\"OBS-DA\", \"Observation-Uncertain\":\"OBS-U\"}\n",
    "    for ent_id, ent in enumerate(sorted(ent_obj_list, key=lambda x: x.start_token_idx)):\n",
    "        # Location-Attribute的label改为其指向的ent的label\n",
    "        if ent.label == \"Location-Attribute\":\n",
    "            target_rel = list(filter(lambda rel_obj: rel_obj.arg1 == ent.brat_id and rel_obj.label == \"located_at\", rel_obj_list))[0]\n",
    "            target_ent = ent_obj_list[ent_obj_list.index(target_rel.arg2)]\n",
    "            ent_label = entity_label_mapper[target_ent.label]\n",
    "        else:\n",
    "            ent_label = entity_label_mapper[ent.label]\n",
    "        output_dict[\"ner\"][ent.sent_idx].append([ent.start_token_idx, ent.end_token_idx, ent_label])\n",
    "        \n",
    "\n",
    "    # Relation\n",
    "    for rel in rel_obj_list:\n",
    "        subj = ent_obj_list[ent_obj_list.index(rel.arg1)]\n",
    "        obj = ent_obj_list[ent_obj_list.index(rel.arg2)]\n",
    "        subj.chain_info[rel.label][\"out\"].append(obj)\n",
    "        obj.chain_info[rel.label][\"in\"].append(subj)\n",
    "        \n",
    "\n",
    "    def get_final_ele_along_chain(curr_ent, final_ents=None, recursive_keys=[(\"part_of\", \"out\")], stop_keys=[]):\n",
    "        if final_ents is None:\n",
    "            final_ents = []\n",
    "        candidate_entities = [ent for k1, k2 in recursive_keys for ent in curr_ent.chain_info[k1][k2]]\n",
    "        if candidate_entities == []:\n",
    "            return final_ents.append(curr_ent)\n",
    "        elif stop_keys and all([True if curr_ent.chain_info[k1][k2] else False for k1, k2 in stop_keys]):\n",
    "            return final_ents.append(curr_ent)\n",
    "        else:\n",
    "            for ent in candidate_entities:\n",
    "                get_final_ele_along_chain(curr_ent=ent, final_ents=final_ents, recursive_keys=[(\"part_of\", \"out\")], stop_keys= [(\"located_at\", \"in\")])\n",
    "        return final_ents\n",
    "\n",
    "    for rel_id, rel in enumerate(sorted(rel_obj_list, key=lambda x: ent_obj_list[ent_obj_list.index(x.arg1)].start_token_idx)):\n",
    "        subj_ent = ent_obj_list[ent_obj_list.index(rel.arg1)]\n",
    "        obj_ent = ent_obj_list[ent_obj_list.index(rel.arg2)]\n",
    "        # 1. 修改rel标签\n",
    "        rel_label = rel.label\n",
    "        if subj_ent.type == \"LOCATT\" or rel.label == \"part_of\":\n",
    "            rel_label = \"modify\"\n",
    "        # 2. 指向传递\n",
    "        transfer_pointing = False\n",
    "        # `a -located_at/suggestive_of-> b -part_of-> c` becomes `a --> c`\n",
    "        # `a(OBS) -loc_at-> b(LOCATT) -loc_at-> c(ANAT) -part_of-> d(ANAT)` becomes `a -loc_at-> d`\n",
    "        if subj_ent.type == \"OBS\" and obj_ent.type == \"ANAT\" and obj_ent.chain_info[\"part_of\"][\"out\"]:\n",
    "            transfer_pointing = True\n",
    "        elif obj_ent.type == \"LOCATT\" and obj_ent.chain_info[\"part_of\"][\"out\"]:\n",
    "            transfer_pointing = True\n",
    "        elif rel.label == \"suggestive_of\" and obj_ent.chain_info[\"part_of\"][\"out\"]:\n",
    "            transfer_pointing = True\n",
    "        \n",
    "        if transfer_pointing:\n",
    "            new_obj_ents = get_final_ele_along_chain(curr_ent=obj_ent, recursive_keys=[(\"part_of\", \"out\")])\n",
    "            for new_obj in new_obj_ents:\n",
    "                output_dict[\"relations\"][subj_ent.sent_idx].append([subj_ent.start_token_idx, subj_ent.end_token_idx, new_obj.start_token_idx, new_obj.end_token_idx, rel_label])\n",
    "        else:\n",
    "            output_dict[\"relations\"][subj_ent.sent_idx].append([subj_ent.start_token_idx, subj_ent.end_token_idx, obj_ent.start_token_idx, obj_ent.end_token_idx, rel_label])\n",
    "\n",
    "    output_path = os.path.join(cxrgraph_output_dir, f\"{dataset_name}-{datasplit}.json\")\n",
    "    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(output_dict))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasplit = \"test\"\n",
    "for dataset_name in [\"MIMIC-CXR\", \"CheXpert\"]:\n",
    "    for file_name in os.listdir(os.path.join(to_be_annotated_dir, dataset_name, \"label_in_use\", datasplit)):\n",
    "        bart2json(dataset_name, datasplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"MIMIC-CXR\"\n",
    "for datasplit in [\"train\", \"dev\"]:\n",
    "    for file_name in os.listdir(os.path.join(to_be_annotated_dir, dataset_name, \"label_in_use\", datasplit)):\n",
    "        bart2json(dataset_name, datasplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve RadGraph to json (the version for ner_re model training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "radgraph_root_dir = \"/Users/liao/Desktop/RadGraph/radgraph-extracting-clinical-entities-and-relations-from-radiology-reports-1.0.0\"\n",
    "\n",
    "radgraph_output_dir = \"./outputs/radgraph/json4ner_re\"\n",
    "\n",
    "if os.path.exists(radgraph_output_dir):\n",
    "    shutil.rmtree(radgraph_output_dir)\n",
    "\n",
    "os.makedirs(radgraph_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sent_relations = {\n",
    "    \"test\": [],\n",
    "    \"test1\": [],\n",
    "    \"train\": [],\n",
    "    \"dev\": [],\n",
    "}\n",
    "\n",
    "for out_file_name in [\"test\", \"test1\", \"train\", \"dev\"]:\n",
    "    if out_file_name == \"test1\":\n",
    "        in_file_name = \"test\"\n",
    "    else:\n",
    "        in_file_name = out_file_name\n",
    "\n",
    "    input_file_path = os.path.join(radgraph_root_dir, f\"{in_file_name}.json\")\n",
    "    with open(input_file_path, \"r\") as f:\n",
    "        docs_dict = json.loads(f.readline())\n",
    "\n",
    "    for doc_key, doc in docs_dict.items():\n",
    "        output_dict = {\n",
    "            \"doc_key\": doc_key,\n",
    "            \"sentences\": [],\n",
    "            \"ner\": [],\n",
    "            \"relations\": [],\n",
    "        }\n",
    "        sent_idx = 0\n",
    "        tokidx2sentidx = []\n",
    "        tokens = doc[\"text\"].split(\" \")\n",
    "        sent = []\n",
    "        for token_id, token in enumerate(tokens):\n",
    "            tokidx2sentidx.append(sent_idx)\n",
    "            sent.append(token)\n",
    "            if token == \".\" or token_id == len(tokens) - 1:\n",
    "                output_dict[\"sentences\"].append(sent)\n",
    "                output_dict[\"ner\"].append([])\n",
    "                output_dict[\"relations\"].append([])\n",
    "                sent_idx += 1\n",
    "                sent = []\n",
    "        assert len(tokens) == len([i for sent in output_dict[\"sentences\"] for i in sent])\n",
    "\n",
    "        if out_file_name == \"test\":\n",
    "            doc_entities = doc[\"labeler_1\"][\"entities\"]\n",
    "        elif out_file_name == \"test1\":\n",
    "            doc_entities = doc[\"labeler_2\"][\"entities\"]\n",
    "        else:\n",
    "            doc_entities = doc[\"entities\"]\n",
    "\n",
    "        for ent_idx, entity in doc_entities.items():\n",
    "            subj_start = entity[\"start_ix\"]\n",
    "            subj_end = entity[\"end_ix\"]\n",
    "            subj_sent_idx = tokidx2sentidx[subj_start]\n",
    "            output_dict[\"ner\"][subj_sent_idx].append([subj_start, subj_end, entity[\"label\"]])\n",
    "            for rel in entity[\"relations\"]:\n",
    "                rel_label = rel[0]\n",
    "                rel_obj_idx = rel[1]\n",
    "                obj_start = doc_entities[rel_obj_idx][\"start_ix\"]\n",
    "                obj_end = doc_entities[rel_obj_idx][\"end_ix\"]\n",
    "                obj_sent_idx = tokidx2sentidx[obj_start]\n",
    "                cross_sent_relations[out_file_name].append(abs(subj_sent_idx - obj_sent_idx))\n",
    "                output_dict[\"relations\"][subj_sent_idx].append([subj_start, subj_end, obj_start, obj_end, rel_label])\n",
    "\n",
    "        output_path = os.path.join(radgraph_output_dir, f\"{out_file_name}.json\")\n",
    "        \n",
    "        if \"test\" in out_file_name:\n",
    "            output_path = os.path.join(radgraph_output_dir, f\"All-{out_file_name}.json\")\n",
    "            with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(output_dict))\n",
    "                f.write(\"\\n\")\n",
    "        if \"txt\" in doc_key and \"test\" in out_file_name:\n",
    "            output_path = os.path.join(radgraph_output_dir, f\"MIMIC-CXR-{out_file_name}.json\")\n",
    "            with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(output_dict))\n",
    "                f.write(\"\\n\")\n",
    "        if \"txt\" not in doc_key and \"test\" in out_file_name:\n",
    "            output_path = os.path.join(radgraph_output_dir, f\"CheXpert-{out_file_name}.json\")\n",
    "            with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(output_dict))\n",
    "                f.write(\"\\n\")\n",
    "        if \"test\" not in out_file_name:\n",
    "            output_path = os.path.join(radgraph_output_dir, f\"MIMIC-CXR-{out_file_name}.json\")\n",
    "            with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(output_dict))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CXRGraph vs RadGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False):\n",
    "    with open(radgraph_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        radgraph_docs = [json.loads(line) for line in f]\n",
    "        \n",
    "    with open(cxrgraph_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        cxrgraph_docs = [json.loads(line) for line in f]\n",
    "        cxrgraph_key2doc = {doc[\"doc_key\"]:doc for doc in cxrgraph_docs}\n",
    "    \n",
    "    \n",
    "    sum_ner, sum_rel = 0, 0\n",
    "    for doc in radgraph_docs:\n",
    "        for sent_ner in doc[\"ner\"]:\n",
    "            sum_ner += len(sent_ner)\n",
    "        for sent_rel in doc[\"relations\"]:\n",
    "            sum_rel += len(sent_rel)\n",
    "\n",
    "    eval_results = {\n",
    "        \"ner\": {\n",
    "            \"num_gt_label\": sum_ner,\n",
    "            \"num_pred_label\": 0,\n",
    "            \"num_correct_label\": 0,\n",
    "        },\n",
    "        \"rel\": {\n",
    "            \"num_gt_label\": sum_rel,\n",
    "            \"num_pred_label\": 0,\n",
    "            \"num_correct_label\": 0,\n",
    "        },\n",
    "        \"rel+\": {\n",
    "            \"num_gt_label\": sum_rel,\n",
    "            \"num_pred_label\": 0,\n",
    "            \"num_correct_label\": 0,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def mix_rel_with_ners(rels, ners):\n",
    "        rels_with_ner = []\n",
    "        for subj_start, subj_end, obj_start, obj_end, rel_label in rels:\n",
    "            subj_ners = list(filter(lambda ner: ner[0] == subj_start and ner[1] == subj_end, ners))\n",
    "            subj_label = subj_ners[0][2] if subj_ners else \"\"\n",
    "            obj_ners = list(filter(lambda ner: ner[0] == obj_start and ner[1] == obj_end, ners))\n",
    "            obj_label = obj_ners[0][2] if obj_ners else \"\"\n",
    "            rels_with_ner.append([subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label])\n",
    "        return rels_with_ner\n",
    "\n",
    "    for rad_doc in radgraph_docs:\n",
    "        cxr_doc = cxrgraph_key2doc[rad_doc[\"doc_key\"]]\n",
    "        \n",
    "        gold_ners = [i for sent in rad_doc[\"ner\"] for i in sent]\n",
    "        pred_ners = [i for sent in cxr_doc[\"ner\"] for i in sent]\n",
    "        \n",
    "        for pred_ner in pred_ners:\n",
    "            eval_results[\"ner\"][\"num_pred_label\"] += 1\n",
    "            if pred_ner in gold_ners:\n",
    "                eval_results[\"ner\"][\"num_correct_label\"] += 1\n",
    "        \n",
    "        \n",
    "        gold_rels = [i for sent in rad_doc[\"relations\"] for i in sent]\n",
    "        gold_rels_with_ner = mix_rel_with_ners(gold_rels, gold_ners)\n",
    "        \n",
    "        pred_rels = [i for sent in cxr_doc[\"relations\"] for i in sent]\n",
    "        pred_rels_with_ner = mix_rel_with_ners(pred_rels, pred_ners)\n",
    "        \n",
    "        for subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label in pred_rels_with_ner:\n",
    "            eval_results[\"rel\"][\"num_pred_label\"] += 1\n",
    "            eval_results[\"rel+\"][\"num_pred_label\"] += 1\n",
    "            if [subj_start, subj_end, obj_start, obj_end, rel_label] in gold_rels:\n",
    "                eval_results[\"rel\"][\"num_correct_label\"] += 1\n",
    "                if [subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label] in gold_rels_with_ner:\n",
    "                    eval_results[\"rel+\"][\"num_correct_label\"] += 1\n",
    "                elif show_diff:\n",
    "                    print(\"rel+ error:\", rad_doc[\"doc_key\"])\n",
    "                    print(\"   \", [subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label])\n",
    "                    print(\"   \", list(filter(lambda rel: rel[0:5] == [subj_start, subj_end, obj_start, obj_end, rel_label], gold_rels_with_ner)))\n",
    "            elif show_diff:\n",
    "                print(\"rel error:\", rad_doc[\"doc_key\"])\n",
    "                print(\"   \", [subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label])\n",
    "                print(\"   \", list(filter(lambda rel: rel[0:4] == [subj_start, subj_end, obj_start, obj_end], gold_rels_with_ner)))\n",
    "\n",
    "    for eval_field, result_dict in eval_results.items():\n",
    "        num_corr = result_dict[\"num_correct_label\"]\n",
    "        num_pred = result_dict[\"num_pred_label\"]\n",
    "        num_gt = result_dict[\"num_gt_label\"]\n",
    "        p = num_corr / num_pred if num_corr > 0 else 0.0\n",
    "        r = num_corr / num_gt if num_corr > 0 else 0.0\n",
    "        f1 = 2 * (p * r) / (p + r) if num_corr > 0 else 0.0\n",
    "        print(f\"[{eval_field}]: P: {p:.5f}, R: {r:.5f}, 【F1: {f1*100:.3f}】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ner]: P: 0.93861, R: 0.98220, 【F1: 95.991】\n",
      "[rel]: P: 0.80160, R: 0.88840, 【F1: 84.277】\n",
      "[rel+]: P: 0.79661, R: 0.88287, 【F1: 83.753】\n"
     ]
    }
   ],
   "source": [
    "radgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/radgraph/json4ner_re/MIMIC-CXR-test.json\"\n",
    "cxrgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4ner_re_radraph_version/MIMIC-CXR-test.json\"\n",
    "cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ner]: P: 0.94601, R: 0.98841, 【F1: 96.674】\n",
      "[rel]: P: 0.80857, R: 0.90111, 【F1: 85.234】\n",
      "[rel+]: P: 0.80658, R: 0.89889, 【F1: 85.024】\n"
     ]
    }
   ],
   "source": [
    "radgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/radgraph/json4ner_re/MIMIC-CXR-test1.json\"\n",
    "cxrgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4ner_re_radraph_version/MIMIC-CXR-test.json\"\n",
    "cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ner]: P: 0.90990, R: 0.93744, 【F1: 92.346】\n",
      "[rel]: P: 0.77432, R: 0.81299, 【F1: 79.319】\n",
      "[rel+]: P: 0.75239, R: 0.78997, 【F1: 77.072】\n"
     ]
    }
   ],
   "source": [
    "radgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/radgraph/json4ner_re/MIMIC-CXR-train.json\"\n",
    "cxrgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4ner_re_radraph_version/MIMIC-CXR-train.json\"\n",
    "cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ner]: P: 0.90625, R: 0.93975, 【F1: 92.270】\n",
      "[rel]: P: 0.76688, R: 0.81136, 【F1: 78.849】\n",
      "[rel+]: P: 0.75188, R: 0.79548, 【F1: 77.306】\n"
     ]
    }
   ],
   "source": [
    "radgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/radgraph/json4ner_re/MIMIC-CXR-dev.json\"\n",
    "cxrgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4ner_re_radraph_version/MIMIC-CXR-dev.json\"\n",
    "cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ner]: P: 0.89934, R: 0.92318, 【F1: 91.110】\n",
      "[rel]: P: 0.72294, R: 0.73828, 【F1: 73.053】\n",
      "[rel+]: P: 0.70736, R: 0.72237, 【F1: 71.479】\n"
     ]
    }
   ],
   "source": [
    "radgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/radgraph/json4ner_re/CheXpert-test.json\"\n",
    "cxrgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4ner_re_radraph_version/CheXpert-test.json\"\n",
    "cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ner]: P: 0.89868, R: 0.91938, 【F1: 90.891】\n",
      "[rel]: P: 0.71515, R: 0.76340, 【F1: 73.849】\n",
      "[rel+]: P: 0.70823, R: 0.75601, 【F1: 73.134】\n"
     ]
    }
   ],
   "source": [
    "radgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/radgraph/json4ner_re/CheXpert-test1.json\"\n",
    "cxrgraph_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4ner_re_radraph_version/CheXpert-test.json\"\n",
    "cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
