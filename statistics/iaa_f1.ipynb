{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve the annotated data\n",
    "\n",
    "We resolve the annotated data from BRAT to json (ner + re)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "\n",
    "1. Set `brat_data_dir` to the annotated data root dir\n",
    "2. Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_annotated_dir = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/to_be_annotated\"\n",
    "output_root_dir = \"./outputs/cxr_graph/json4iaa\"\n",
    "\n",
    "brat_data_dir_labeler1 = \"/Users/liao/myProjects/repo/brat/data/structured_reporting/ours/liao\"\n",
    "brat_data_dir_labeler2 = \"/Users/liao/myProjects/repo/remote_brat/data/structured_reporting/ours/xiang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(output_root_dir):\n",
    "    shutil.rmtree(output_root_dir)\n",
    "\n",
    "os.makedirs(output_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolve BRAT result to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnEntityClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # T0\n",
    "        self.label = \"\"\n",
    "        self.start_index = -1  # include (char idx)\n",
    "        self.end_index = -1  # not include\n",
    "        self.token_str = \"\"\n",
    "        self.att_objs = []\n",
    "\n",
    "        self.id = \"\"  # E0\n",
    "        self.start_token_idx = -1 # include\n",
    "        self.end_token_idx = -1 # include\n",
    "        self.sent_idx = -1\n",
    "\n",
    "        self.type = \"\"  # ANAT, OBS, LOCATT\n",
    "        self.chain_info = {\n",
    "            \"modify\": {\"in\": [], \"out\": []},\n",
    "            \"part_of\": {\"in\": [], \"out\": []},\n",
    "            \"located_at\": {\"in\": [], \"out\": []},\n",
    "            \"suggestive_of\": {\"in\": [], \"out\": []},\n",
    "        }\n",
    "        self.resolve(stripped_str)\n",
    "        \n",
    "        self.abnormality = \"NA\"\n",
    "        self.action = \"NA\"\n",
    "        self.evolution = \"NA\"\n",
    "        \n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.brat_id}\\t{self.label} {self.start_index} {self.end_index}\\t{self.token_str}\\n\"\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(T\\d+)\\t(.+) (\\d+) (\\d+)\\t(.+)\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        obs_labels = [\"Observation-Present\", \"Observation-Absent\", \"Observation-Uncertain\"]\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, start_index, end_index, self.token_str = match_obj.groups()\n",
    "            self.start_index = int(start_index)\n",
    "            self.end_index = int(end_index)\n",
    "            if self.label in obs_labels:\n",
    "                self.type = \"OBS\"\n",
    "            elif self.label == \"Anatomy\":                \n",
    "                self.type = \"ANAT\"\n",
    "            elif self.label == \"Location-Attribute\":\n",
    "                self.type = \"LOCATT\"\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot identify: {self.label}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnEntityClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)\n",
    "\n",
    "\n",
    "class AnnRelationClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # R0\n",
    "        self.label = \"\"\n",
    "        self.arg1 = \"\"  # from entity: T0\n",
    "        self.arg2 = \"\"  # to entity: T1\n",
    "        self.resolve(stripped_str)\n",
    "\n",
    "        self.id = \"\"  # R0\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        return f\"{self.brat_id}\\t{self.label} Arg1:{self.arg1} Arg2:{self.arg2}\\t\\n\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(R\\d+)\\t(.+) Arg1:(T\\d+) Arg2:(T\\d+)\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, self.arg1, self.arg2 = match_obj.groups()\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnRelationClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)\n",
    "\n",
    "\n",
    "class AnnAttributeClass:\n",
    "    def __init__(self, stripped_str) -> None:\n",
    "        self.brat_id = \"\"  # A0\n",
    "        self.label = \"\"\n",
    "        self.value = \"\"\n",
    "        self.target_entity_id = \"\"  # T0\n",
    "        \n",
    "        self.resolve(stripped_str)\n",
    "\n",
    "    def get_ann_str(self) -> str:\n",
    "        if self.value:\n",
    "            return f\"{self.brat_id}\\t{self.label} {self.target_entity_id} {self.value}\"\n",
    "        else:\n",
    "            return f\"{self.brat_id}\\t{self.label} {self.target_entity_id}\"\n",
    "\n",
    "    def get_json_str(self) -> str:\n",
    "        if self.label == \"isAbnormal_OBS\":\n",
    "            return \"is_abnormal\"\n",
    "        if self.label == \"isNormal_OBS\":\n",
    "            return \"is_normal\"\n",
    "        if self.label == \"Uncertian_Tendency\":\n",
    "            return f\"uncertainty:{self.value}\"\n",
    "            raise ValueError(\"Should not have this attribute\")\n",
    "        if self.label == \"isRelative_Modifier\":\n",
    "            return f\"is_relative_modifier:{self.value}\"\n",
    "        if self.label == \"show_RelativeChange\":\n",
    "            return f\"has_relative_change:{self.value}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.get_ann_str()\n",
    "\n",
    "    def resolve(self, stripped_str):\n",
    "        patten = r\"(A\\d+)\\t(.+) (T\\d+) ?(.+)?\"\n",
    "        match_obj = re.match(patten, stripped_str)\n",
    "        if match_obj:\n",
    "            self.brat_id, self.label, self.target_entity_id, self.value = match_obj.groups()\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot resolve: {stripped_str}\")\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, AnnAttributeClass):\n",
    "            return self.brat_id == other.brat_id\n",
    "        else:\n",
    "            return other == self.brat_id or other == self.target_entity_id\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.brat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bart2json(brat_data_dir, dataset_names, datasplits, output_name):\n",
    "    for dataset_name in dataset_names: # , \"CheXpert\"\n",
    "        for datasplit in datasplits:\n",
    "            for file_name in os.listdir(os.path.join(to_be_annotated_dir, dataset_name, \"label_in_use\", datasplit)):\n",
    "                if dataset_name == \"MIMIC-CXR\":\n",
    "                    doc_key = file_name.lstrip(f\"{dataset_name}_\").replace(\"_\", \"/\")\n",
    "                if dataset_name == \"CheXpert\":\n",
    "                    doc_key = file_name.lstrip(f\"{dataset_name}_\").rstrip(\".txt\")\n",
    "\n",
    "                txt_file_name = file_name\n",
    "                ann_file_name = f'{file_name.rstrip(\".txt\")}.ann'\n",
    "\n",
    "                txt_file = os.path.join(brat_data_dir, dataset_name, datasplit, txt_file_name)\n",
    "                ann_file = os.path.join(brat_data_dir, dataset_name, datasplit, ann_file_name)\n",
    "\n",
    "                output_dict = {\n",
    "                    \"doc_key\": doc_key,\n",
    "                    \"sentences\": [],\n",
    "                    \"ner\": [],\n",
    "                    \"relations\": [],\n",
    "                    \"entity_attributes\": [],\n",
    "                }\n",
    "\n",
    "                # 读取原始doc：只读取第一行\n",
    "                with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    doc_str = f.readline().strip()\n",
    "\n",
    "                # 超过这个范围的标签都应该排除（因为我们把RadGraph的标签也一起呈现给了标注者，所以解析时需要排除这些已有的标签）\n",
    "                valid_doc_len = len(doc_str)\n",
    "\n",
    "                # 读取标签\n",
    "                with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    ann_lines = f.readlines()\n",
    "                    # print(ann_lines)\n",
    "\n",
    "                ent_obj_list = []\n",
    "                rel_obj_list = []\n",
    "                att_obj_list = []\n",
    "                for ann_line in ann_lines:\n",
    "                    stripped_ann_line = ann_line.strip()\n",
    "                    if stripped_ann_line.startswith(\"T\"):\n",
    "                        ent = AnnEntityClass(stripped_ann_line)\n",
    "                        ent_obj_list.append(ent)\n",
    "                    elif stripped_ann_line.startswith(\"R\"):\n",
    "                        rel = AnnRelationClass(stripped_ann_line)\n",
    "                        rel_obj_list.append(rel)\n",
    "                    elif stripped_ann_line.startswith(\"A\"):\n",
    "                        att = AnnAttributeClass(stripped_ann_line)\n",
    "                        att_obj_list.append(att)\n",
    "                        ent = ent_obj_list[ent_obj_list.index(att.target_entity_id)]\n",
    "                        ent.att_objs.append(att)\n",
    "                        if att.label == \"isAbnormal_OBS\":\n",
    "                            ent.abnormality = \"Abnormal\"\n",
    "                        if att.label == \"isNormal_OBS\":\n",
    "                            ent.abnormality = \"Normal\"\n",
    "                        if att.label == \"isRelative_Modifier\":\n",
    "                            ent.action = att.value\n",
    "                        if att.label == \"show_RelativeChange\":\n",
    "                            ent.evolution = att.value\n",
    "                    else:\n",
    "                        raise ValueError(f\"Uncatched value from .ann file: {stripped_ann_line}\")\n",
    "                \n",
    "                ent_obj_list = list(filter(lambda ent: ent.start_index <= valid_doc_len and ent.end_index <= valid_doc_len, ent_obj_list))\n",
    "                rel_obj_list = list(filter(lambda rel: rel.arg1 in ent_obj_list and rel.arg2 in ent_obj_list, rel_obj_list))\n",
    "                att_obj_list = list(filter(lambda att: att.target_entity_id in ent_obj_list in ent_obj_list, att_obj_list))\n",
    "\n",
    "                # 识别token的位置，并添加token_idx; 按句子拆分\n",
    "                doc_tokens = doc_str.split(\" \")\n",
    "                token_start_idx_list = [] # token first char\n",
    "                token_end_idx_list = [] # token last char + 1\n",
    "                curr_start = 0\n",
    "                \n",
    "                sent_idx = 0\n",
    "                tokidx2sentidx = []\n",
    "                sent = []\n",
    "                for tok_idx, token_str in enumerate(doc_tokens):\n",
    "                    # 识别token的位置，并添加token_idx\n",
    "                    token_start_idx_list.append(curr_start)\n",
    "                    token_end_idx_list.append(curr_start + len(token_str))\n",
    "                    curr_start += len(token_str) + 1 # whitespace\n",
    "                    \n",
    "                    # 按句子拆分\n",
    "                    tokidx2sentidx.append(sent_idx)\n",
    "                    sent.append(token_str)\n",
    "                    if token_str == \".\" or tok_idx == len(doc_tokens) - 1:\n",
    "                        output_dict[\"sentences\"].append(sent)\n",
    "                        output_dict[\"ner\"].append([])\n",
    "                        output_dict[\"relations\"].append([])\n",
    "                        output_dict[\"entity_attributes\"].append([])\n",
    "                        sent_idx += 1\n",
    "                        sent = []\n",
    "                assert len(doc_tokens) == len([i for sent in output_dict[\"sentences\"] for i in sent])\n",
    "\n",
    "                for ent in ent_obj_list:\n",
    "                    ent.start_token_idx = token_start_idx_list.index(ent.start_index)\n",
    "                    ent.end_token_idx = token_end_idx_list.index(ent.end_index)\n",
    "                    assert ent.token_str == \" \".join(doc_tokens[ent.start_token_idx : ent.end_token_idx + 1])\n",
    "                    \n",
    "                    starttok_sent_idx = tokidx2sentidx[ent.start_token_idx]\n",
    "                    endtok_sent_idx = tokidx2sentidx[ent.end_token_idx]\n",
    "                    ent.sent_idx = starttok_sent_idx\n",
    "                    assert starttok_sent_idx == endtok_sent_idx\n",
    "\n",
    "                # Entity\n",
    "                for ent_id, ent in enumerate(sorted(ent_obj_list, key=lambda x: x.start_token_idx)):\n",
    "                    output_dict[\"ner\"][ent.sent_idx].append([ent.start_token_idx, ent.end_token_idx, ent.label])\n",
    "                    \n",
    "                    # Attribute\n",
    "                    if ent.att_objs:\n",
    "                        output_dict[\"entity_attributes\"][ent.sent_idx].append([ent.start_token_idx, ent.end_token_idx, ent.abnormality, ent.action, ent.evolution])\n",
    "\n",
    "                # Relation\n",
    "                for rel_id, rel in enumerate(sorted(rel_obj_list, key=lambda x: ent_obj_list[ent_obj_list.index(x.arg1)].start_token_idx)):\n",
    "                    subj = ent_obj_list[ent_obj_list.index(rel.arg1)]\n",
    "                    obj = ent_obj_list[ent_obj_list.index(rel.arg2)]\n",
    "                    output_dict[\"relations\"][subj.sent_idx].append([subj.start_token_idx, subj.end_token_idx, obj.start_token_idx, obj.end_token_idx, rel.label])\n",
    "                \n",
    "\n",
    "                output_path = os.path.join(output_root_dir, output_name)\n",
    "                with open(output_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(json.dumps(output_dict))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart2json(brat_data_dir=brat_data_dir_labeler1, dataset_names=[\"MIMIC-CXR\"], datasplits=[\"test\"], output_name=\"test1.json\")\n",
    "bart2json(brat_data_dir=brat_data_dir_labeler2, dataset_names=[\"MIMIC-CXR\"], datasplits=[\"test\"], output_name=\"test2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def cxr_vs_rad(radgraph_path, cxrgraph_path, show_diff=False):\n",
    "    with open(radgraph_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        radgraph_docs = [json.loads(line) for line in f]\n",
    "        \n",
    "    with open(cxrgraph_path, \"r\", encoding=\"UTF-8\") as f:\n",
    "        cxrgraph_docs = [json.loads(line) for line in f]\n",
    "        cxrgraph_key2doc = {doc[\"doc_key\"]:doc for doc in cxrgraph_docs}\n",
    "    \n",
    "    \n",
    "    sum_ner, sum_rel, sum_attr = 0, 0, 0\n",
    "    for doc in radgraph_docs:\n",
    "        for sent_ner in doc[\"ner\"]:\n",
    "            sum_ner += len(sent_ner)\n",
    "        for sent_rel in doc[\"relations\"]:\n",
    "            sum_rel += len(sent_rel)\n",
    "        for sent_attr in doc[\"entity_attributes\"]:\n",
    "            for attr in sent_attr:\n",
    "                if attr[2] != \"NA\":\n",
    "                    sum_attr += 1\n",
    "                if attr[3] != \"NA\":\n",
    "                    sum_attr += 1\n",
    "                if attr[4] != \"NA\":\n",
    "                    sum_attr += 1\n",
    "\n",
    "    eval_results = {\n",
    "        \"ner\": {\n",
    "            \"num_gt_label\": sum_ner,\n",
    "            \"num_pred_label\": 0,\n",
    "            \"num_correct_label\": 0,\n",
    "        },\n",
    "        \"rel\": {\n",
    "            \"num_gt_label\": sum_rel,\n",
    "            \"num_pred_label\": 0,\n",
    "            \"num_correct_label\": 0,\n",
    "        },\n",
    "        \"rel+\": {\n",
    "            \"num_gt_label\": sum_rel,\n",
    "            \"num_pred_label\": 0,\n",
    "            \"num_correct_label\": 0,\n",
    "        },\n",
    "        \"attr\": {\n",
    "            \"num_gt_label\": sum_attr,\n",
    "            \"num_pred_label\": 0,\n",
    "            \"num_correct_label\": 0,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def mix_rel_with_ners(rels, ners):\n",
    "        rels_with_ner = []\n",
    "        for subj_start, subj_end, obj_start, obj_end, rel_label in rels:\n",
    "            subj_ners = list(filter(lambda ner: ner[0] == subj_start and ner[1] == subj_end, ners))\n",
    "            subj_label = subj_ners[0][2] if subj_ners else \"\"\n",
    "            obj_ners = list(filter(lambda ner: ner[0] == obj_start and ner[1] == obj_end, ners))\n",
    "            obj_label = obj_ners[0][2] if obj_ners else \"\"\n",
    "            rels_with_ner.append([subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label])\n",
    "        return rels_with_ner\n",
    "\n",
    "    for rad_doc in radgraph_docs:\n",
    "        cxr_doc = cxrgraph_key2doc[rad_doc[\"doc_key\"]]\n",
    "        \n",
    "        # Ent\n",
    "        gold_ners = [i for sent in rad_doc[\"ner\"] for i in sent]\n",
    "        pred_ners = [i for sent in cxr_doc[\"ner\"] for i in sent]\n",
    "        \n",
    "        for pred_ner in pred_ners:\n",
    "            eval_results[\"ner\"][\"num_pred_label\"] += 1\n",
    "            if pred_ner in gold_ners:\n",
    "                eval_results[\"ner\"][\"num_correct_label\"] += 1\n",
    "        \n",
    "        # Attr\n",
    "        gold_attrs = [i for sent in rad_doc[\"entity_attributes\"] for i in sent]\n",
    "        pred_attrs = [i for sent in cxr_doc[\"entity_attributes\"] for i in sent]\n",
    "        gold_attrs_checklist = [(i[0], i[1]) for sent in rad_doc[\"entity_attributes\"] for i in sent]\n",
    "                \n",
    "        for pred_attr in pred_attrs:\n",
    "            pred_attr_indices = (pred_attr[0], pred_attr[1])\n",
    "            pred_attr_normality = pred_attr[2]\n",
    "            pred_attr_action = pred_attr[3]\n",
    "            pred_attr_evolution = pred_attr[4]\n",
    "            \n",
    "            if pred_attr_normality != \"NA\":\n",
    "                eval_results[\"attr\"][\"num_pred_label\"] += 1\n",
    "            if pred_attr_action != \"NA\":\n",
    "                eval_results[\"attr\"][\"num_pred_label\"] += 1\n",
    "            if pred_attr_evolution != \"NA\":\n",
    "                eval_results[\"attr\"][\"num_pred_label\"] += 1\n",
    "            \n",
    "            if pred_attr_indices in gold_attrs_checklist:\n",
    "                _, _, gold_attr_normality, gold_attr_action, gold_attr_evolution = gold_attrs[gold_attrs_checklist.index(pred_attr_indices)]\n",
    "                \n",
    "                if gold_attr_normality != \"NA\" and pred_attr_normality == gold_attr_normality:\n",
    "                    eval_results[\"attr\"][\"num_correct_label\"] += 1\n",
    "                if gold_attr_action != \"NA\" and pred_attr_action == gold_attr_action:\n",
    "                    eval_results[\"attr\"][\"num_correct_label\"] += 1\n",
    "                if gold_attr_evolution != \"NA\" and pred_attr_evolution == gold_attr_evolution:\n",
    "                    eval_results[\"attr\"][\"num_correct_label\"] += 1\n",
    "        \n",
    "        # Rel\n",
    "        gold_rels = [i for sent in rad_doc[\"relations\"] for i in sent]\n",
    "        gold_rels_with_ner = mix_rel_with_ners(gold_rels, gold_ners)\n",
    "        \n",
    "        pred_rels = [i for sent in cxr_doc[\"relations\"] for i in sent]\n",
    "        pred_rels_with_ner = mix_rel_with_ners(pred_rels, pred_ners)\n",
    "        \n",
    "        for subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label in pred_rels_with_ner:\n",
    "            eval_results[\"rel\"][\"num_pred_label\"] += 1\n",
    "            eval_results[\"rel+\"][\"num_pred_label\"] += 1\n",
    "            if [subj_start, subj_end, obj_start, obj_end, rel_label] in gold_rels:\n",
    "                eval_results[\"rel\"][\"num_correct_label\"] += 1\n",
    "                if [subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label] in gold_rels_with_ner:\n",
    "                    eval_results[\"rel+\"][\"num_correct_label\"] += 1\n",
    "                elif show_diff:\n",
    "                    print(\"rel+ error:\", rad_doc[\"doc_key\"])\n",
    "                    print(\"   \", [subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label])\n",
    "                    print(\"   \", list(filter(lambda rel: rel[0:5] == [subj_start, subj_end, obj_start, obj_end, rel_label], gold_rels_with_ner)))\n",
    "            elif show_diff:\n",
    "                print(\"rel error:\", rad_doc[\"doc_key\"])\n",
    "                print(\"   \", [subj_start, subj_end, obj_start, obj_end, rel_label, subj_label, obj_label])\n",
    "                print(\"   \", list(filter(lambda rel: rel[0:4] == [subj_start, subj_end, obj_start, obj_end], gold_rels_with_ner)))\n",
    "\n",
    "    for eval_field, result_dict in eval_results.items():\n",
    "        num_corr = result_dict[\"num_correct_label\"]\n",
    "        num_pred = result_dict[\"num_pred_label\"]\n",
    "        num_gt = result_dict[\"num_gt_label\"]\n",
    "        p = num_corr / num_pred if num_corr > 0 else 0.0\n",
    "        r = num_corr / num_gt if num_corr > 0 else 0.0\n",
    "        f1 = 2 * (p * r) / (p + r) if num_corr > 0 else 0.0\n",
    "        print(f\"[{eval_field}]: P: {p:.5f}, R: {r:.5f}, 【F1: {f1*100:.3f}】\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ner]: P: 0.97205, R: 0.97280, 【F1: 97.243】\n",
      "[rel]: P: 0.93936, R: 0.93626, 【F1: 93.781】\n",
      "[rel+]: P: 0.91180, R: 0.90879, 【F1: 91.029】\n",
      "[attr]: P: 0.90625, R: 0.84302, 【F1: 87.349】\n"
     ]
    }
   ],
   "source": [
    "labeler1_json_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4iaa/test1.json\"\n",
    "labeler2_json_path = \"/Users/liao/myProjects/VSCode_workspace/cxr_graph/graph_annotation_process/outputs/cxr_graph/json4iaa/test2.json\"\n",
    "cxr_vs_rad(labeler1_json_path, labeler2_json_path, show_diff=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbe186fad143582492f874971b555a6a67ca040c11267037e80d88fc47d0fa6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
